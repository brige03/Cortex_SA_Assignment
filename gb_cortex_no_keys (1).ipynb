{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c035aa37-c270-4378-b4eb-adbf42243043",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e81969-d28d-46cd-a110-6ad55483a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import http.client\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import NoCredentialsError\n",
    "except ImportError as e:\n",
    "    boto3 = None\n",
    "    print(\"⚠️ boto3 failed to import, possibly due to SSL issues or environment problems:\", e)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# === Configuration ===\n",
    "\n",
    "XSIAM_BASE_URL = \"https://api-cribldev.xdr.us.paloaltonetworks.com\"\n",
    "S3_BUCKET_NAME = \"gbcortex\"\n",
    "\n",
    "# Harcoded for demo, but ideally should be stored in environment variables in production\n",
    "API_KEY = \"KEY HAS BEEN TAKING OUT\" # Replace with your actual API key\n",
    "XDR_AUTH_ID = \"ID HAS BEEN TAKING OOUT\" # API key ID for x-xdr-auth-id header. Replace with your actual ID\n",
    "\n",
    "AWS_REGION = \"us-east-2\"  # Default region\n",
    "# Harcoded for demo, but ideally should be stored in environment variables in production\n",
    "AWS_ACCESS_KEY_ID = \"KEY HAS BEEN TAKING OUT\" # Replace with your actual key\n",
    "AWS_SECRET_ACCESS_KEY = \"KEY HAS BEEN TAKING OUT\" # Replace with your actual key\n",
    "\n",
    "#------ This section for establishing connection to S3 bucket and getting API headers from Cortex XSIAM\n",
    "if boto3:\n",
    "    try:\n",
    "        # Initialize S3 client. Boto3 automatically looks for credentials in environment variables or ~/.aws/credentials.\n",
    "        s3_client = boto3.client(\n",
    "            's3',\n",
    "            region_name=AWS_REGION,\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "        )\n",
    "\n",
    "        print(f\"S3 Client is set for bucket: '{S3_BUCKET_NAME}'\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to initialize boto3 S3 client. Ensure AWS credentials and region are configured.\", e)\n",
    "        s3_client = None\n",
    "else:\n",
    "    s3_client = None\n",
    "\n",
    "def get_headers(api_key: str, auth_id: str = None) -> dict:\n",
    "    \"\"\"Generates standard API headers including Authorization and optional x-xdr-auth-id.\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": api_key,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    if auth_id:\n",
    "        headers['x-xdr-auth-id'] = auth_id\n",
    "    return headers\n",
    "\n",
    "# Print generated headers for verification\n",
    "print(\"Alerts API Headers:\", get_headers(API_KEY, XDR_AUTH_ID))\n",
    "\n",
    "#Date object conversion for later fetching the alerts within certain time frame\n",
    "def to_millis(date_obj: datetime) -> int:\n",
    "    \"\"\"Converts a datetime object to milliseconds since epoch.\"\"\"\n",
    "    return int(date_obj.timestamp() * 1000)\n",
    "    \n",
    "#--------This section clears json objects within targeted S3 bucket before loading the latest alerts into it\n",
    "def clear_s3_alerts_folder(bucket_name: str, s3_client):\n",
    "    \"\"\"Deletes all objects within the 'alerts/' prefix in the S3 bucket.\"\"\"\n",
    "    if not s3_client:\n",
    "        print(\"❌ S3 client not initialized. Skipping S3 folder clearing.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Attempting to clear alerts folder in s3://{bucket_name}/alerts/ ---\")\n",
    "    try:\n",
    "        objects_to_delete = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        pages = paginator.paginate(Bucket=bucket_name, Prefix='alerts/')\n",
    "\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                for obj in page['Contents']:\n",
    "                    objects_to_delete.append({'Key': obj['Key']})\n",
    "\n",
    "        if objects_to_delete:\n",
    "            print(f\"Found {len(objects_to_delete)} objects to delete in s3://{bucket_name}/alerts/\")\n",
    "            for i in range(0, len(objects_to_delete), 1000):\n",
    "                batch = objects_to_delete[i:i+1000]\n",
    "                delete_keys = {'Objects': batch}\n",
    "                print(f\"  Deleting batch {i//1000 + 1} of {len(batch)} objects...\")\n",
    "                response = s3_client.delete_objects(\n",
    "                    Bucket=bucket_name,\n",
    "                    Delete=delete_keys\n",
    "                )\n",
    "                if 'Errors' in response:\n",
    "                    print(f\"❌ Errors during batch delete: {response['Errors']}\")\n",
    "                else:\n",
    "                    print(f\"  Batch delete successful. Deleted {len(batch)} objects.\")\n",
    "            print(\"✅ Finished clearing alerts folder.\")\n",
    "        else:\n",
    "            print(\"No objects found in s3://{bucket_name}/alerts/ to clear.\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(f\"❌ AWS credentials not found. S3 folder clearing failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during S3 folder clearing: {e}\")\n",
    "    print(\"--- S3 folder clearing process finished ---\")\n",
    "\n",
    "# --- Section for fetching alerts from Cortex XSIAM ---\n",
    "def fetch_alerts_page(api_key: str, next_page_token: str = None) -> dict:\n",
    "    \"\"\"Fetches a single page of alerts from the Cortex XSIAM Incident Management API.\"\"\"\n",
    "    \n",
    "    # Setting correct API endpoint url for retriving alerts per Cortex XSIAM Incident Management doc\n",
    "    #https://docs-cortex.paloaltonetworks.com/r/Cortex-XDR-REST-API/Get-Alerts-Multi-Events-v2\n",
    "    url = f\"{XSIAM_BASE_URL}/public_api/v2/alerts/get_alerts_multi_events/\"\n",
    "\n",
    "    #Setting up payload request\n",
    "    payload_request_data = {\n",
    "        \"use_page_token\": True, # Use page token for pagination\n",
    "        \"search_to\": 100, # Request maximum 100 results per page\n",
    "        \"sort\": {\"field\": \"creation_time\", \"keyword\": \"desc\"},\n",
    "        # Filters are temporarily removed for debugging the 500 error.\n",
    "        # Re-add filters later if the base call with pagination works.\n",
    "        # \"filters\": [{\n",
    "        #     \"field\": \"creation_time\",\n",
    "        #     \"operator\": \"in_range\",\n",
    "        #     \"value\": {\n",
    "        #          \"start\": to_millis(datetime.now() - timedelta(days=30)),\n",
    "        #          \"end\": to_millis(datetime.now())\n",
    "        #     }\n",
    "        # }]\n",
    "    }\n",
    "\n",
    "    # Adding next_page_token to payload if provided (for subsequent pages)\n",
    "    if next_page_token:\n",
    "        payload_request_data[\"next_page_token\"] = next_page_token\n",
    "\n",
    "    payload = {\"request_data\": payload_request_data}\n",
    "\n",
    "    print(f\"Fetching alerts with payload: {json.dumps(payload)}\")\n",
    "\n",
    "    # Use headers with x-xdr-auth-id for v2 alerts endpoint\n",
    "    headers = get_headers(api_key, XDR_AUTH_ID)\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Return the full response data\n",
    "    return response.json()\n",
    "\n",
    "def upload_alert_to_s3(alert: dict, bucket_name: str, s3_client):\n",
    "    \"\"\"Uploads a single alert dictionary as a JSON object to an S3 bucket.\"\"\"\n",
    "    if not s3_client:\n",
    "        print(\"❌ S3 client not initialized. Skipping S3 upload.\")\n",
    "        return\n",
    "\n",
    "    # Generate a unique object key (filename) for the alert in S3 using alert_id\n",
    "    # This allows overwriting previous versions if the script is run again.\n",
    "    key = f\"unknown_alert_{datetime.now().strftime('%Y%m%d%H%M%S%f')}\"\n",
    "    alert_id = alert.get('alert_id', key) # Use a default if alert_id is missing, add timestamp for uniqueness in this case\n",
    "    object_key = f\"alerts/{alert_id}.json\"\n",
    "\n",
    "    # Convert the alert dictionary to a JSON string\n",
    "    try:\n",
    "        alert_json_data = json.dumps(alert, indent=2)\n",
    "    except TypeError as e:\n",
    "        print(f\"❌ Failed to serialize alert {alert_id} to JSON: {e}\")\n",
    "        print(f\"Alert data: {alert}\") # Log the problematic alert data\n",
    "        return\n",
    "\n",
    "    # Upload the JSON data to S3\n",
    "    try:\n",
    "        print(f\"⬆️ Attempting to upload alert {alert_id} to s3://{bucket_name}/{object_key}\")\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=object_key,\n",
    "            Body=alert_json_data,\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "        print(f\"✅ Successfully uploaded alert {alert_id} to S3.\")\n",
    "    except NoCredentialsError:\n",
    "        print(f\"❌ AWS credentials not found for alert {alert_id}. S3 upload failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to upload alert {alert_id} to S3: {e}\")\n",
    "\n",
    "def fetch_all_alerts(api_key: str):\n",
    "    \"\"\"Fetches all alerts page by page and uploads them to S3.\"\"\"\n",
    "    next_page_token = None\n",
    "    page_count = 0\n",
    "    total_uploaded_count = 0\n",
    "\n",
    "    print(\"\\n--- Starting alert fetching and S3 upload process ---\")\n",
    "\n",
    "    while True:\n",
    "        print(f\"Fetching page: {page_count}...\")\n",
    "        try:\n",
    "            response_data = fetch_alerts_page(api_key, next_page_token=next_page_token)\n",
    "\n",
    "            if response_data and 'reply' in response_data and 'alerts' in response_data['reply'] is not None:\n",
    "                alerts_on_page = response_data['reply']['alerts']\n",
    "                page_alert_count = len(alerts_on_page)\n",
    "                print(f\"Fetched {page_alert_count} alerts on page {page_count}.\")\n",
    "\n",
    "                if alerts_on_page:\n",
    "                    print(f\"--- Starting S3 upload for {page_alert_count} alerts on page {page_count} ---\")\n",
    "                    for i, alert in enumerate(alerts_on_page):\n",
    "                        print(f\"  Processing alert {i + 1}/{page_alert_count} on page {page_count}...\")\n",
    "                        upload_alert_to_s3(alert, S3_BUCKET_NAME, s3_client)\n",
    "                        total_uploaded_count += 1\n",
    "                    print(f\"--- Finished S3 upload for alerts on page {page_count} ---\")\n",
    "\n",
    "                # Get the next page token from the response\n",
    "                next_page_token = response_data['reply'].get('next_page_token')\n",
    "                print(f\"Next page token for page {page_count}: {next_page_token}\")\n",
    "\n",
    "                # If there is no next page token, break the loop\n",
    "                if not next_page_token or page_alert_count == 0: # Also break if a page returns 0 alerts (can happen on last page)\n",
    "                    print(\"No more pages to fetch or no alerts on the last page.\")\n",
    "                    break\n",
    "\n",
    "                page_count += 1\n",
    "                # Optional: Add a small delay between requests to avoid rate limiting\n",
    "                time.sleep(1) \n",
    "\n",
    "            else:\n",
    "                 print(f\"❌ Unexpected response structure or no 'reply'/'alerts' in response on page {page_count}: {response_data}\")\n",
    "                 break # Exit loop on unexpected response\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"❌ Request failed while fetching page {page_count}: {e}\")\n",
    "            break # Exit loop on request error\n",
    "        except Exception as e:\n",
    "            print(f\"❌ An unexpected error occurred while fetching page {page_count}: {e}\")\n",
    "            break # Exit loop on other errors\n",
    "\n",
    "    print(f\"--- Finished alert fetching and S3 upload. Total alerts uploaded: {total_uploaded_count} ---\")\n",
    "\n",
    "def verify_alerts_in_s3(bucket_name: str, s3_client):\n",
    "    \"\"\"Lists objects in the 'alerts/' prefix of the S3 bucket and reports the count.\"\"\"\n",
    "    if not s3_client:\n",
    "        print(\"❌ S3 client not initialized. Skipping S3 verification.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Verifying alerts in S3 bucket '{bucket_name}' ---\")\n",
    "    object_count = 0\n",
    "    try:\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        # List objects within the 'alerts/' prefix\n",
    "        pages = paginator.paginate(Bucket=bucket_name, Prefix='alerts/')\n",
    "\n",
    "        for page in pages:\n",
    "            if 'Contents' in page:\n",
    "                object_count += len(page['Contents'])\n",
    "\n",
    "        print(f\"✅ Found {object_count} alert objects in s3://{bucket_name}/alerts/\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(f\"❌ AWS credentials not found. S3 verification failed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during S3 verification: {e}\")\n",
    "\n",
    "    print(\"--- S3 verification complete ---\")\n",
    "\n",
    "# === Main Execution ===\n",
    "\n",
    "# Clear existing alerts folder in S3 for demo purposes\n",
    "clear_s3_alerts_folder(S3_BUCKET_NAME, s3_client)\n",
    "\n",
    "# Fetch all alerts and upload to S3\n",
    "fetch_all_alerts(API_KEY)\n",
    "\n",
    "# Verify the number of alerts uploaded to S3\n",
    "verify_alerts_in_s3(S3_BUCKET_NAME, s3_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
